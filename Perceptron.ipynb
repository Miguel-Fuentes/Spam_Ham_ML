{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os #for getting data from OS\n",
    "import collections #imported for Counter\n",
    "import re #regex for bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET 1---------------------------------\n",
      "\n",
      "Data initialized:\n",
      "Started training...\n",
      "\n",
      "Best results from hyperparameter tuning:\n",
      "Best Acc:  100.0\n",
      "best Epoch:  75\n",
      "Best lr:  0.01\n",
      "\n",
      "RESULTS FROM ENTIRE DATASET:\n",
      "\n",
      "Learning rate: 0.0100\n",
      "Number of epochs: 75\n",
      "Emails classified correctly: 442/478\n",
      "Accuracy: 92.4686%\n",
      "\n",
      "DATASET 2---------------------------------\n",
      "\n",
      "Data initialized:\n",
      "Started training...\n",
      "\n",
      "Best results from hyperparameter tuning:\n",
      "Best Acc:  100.0\n",
      "best Epoch:  75\n",
      "Best lr:  0.01\n",
      "\n",
      "RESULTS FROM ENTIRE DATASET:\n",
      "\n",
      "Learning rate: 0.0100\n",
      "Number of epochs: 75\n",
      "Emails classified correctly: 418/456\n",
      "Accuracy: 91.6667%\n",
      "\n",
      "DATASET 3---------------------------------\n",
      "\n",
      "Data initialized:\n",
      "Started training...\n",
      "\n",
      "Best results from hyperparameter tuning:\n",
      "Best Acc:  100.0\n",
      "best Epoch:  75\n",
      "Best lr:  0.01\n",
      "\n",
      "RESULTS FROM ENTIRE DATASET:\n",
      "\n",
      "Learning rate: 0.0100\n",
      "Number of epochs: 75\n",
      "Emails classified correctly: 492/543\n",
      "Accuracy: 90.6077%\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = {'w0': 0.5}\n",
    "        self.bias = 0.1\n",
    "        self.trainingSet = {}\n",
    "        self.trainSet70 = {}\n",
    "        self.trainSet30 = {}\n",
    "        self.testSet = {}\n",
    "        self.classes = [\"ham\", \"spam\"]\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.01\n",
    "        \n",
    "    def bagOfWords(self, text):\n",
    "        '''\n",
    "        Returns a dictionary where key is word and value is the \n",
    "        count of that word in the text passed in\n",
    "        '''\n",
    "        return dict(collections.Counter(re.findall(r'\\w+', text)))\n",
    "    \n",
    "    def getData(self, data, directory, trueClass):\n",
    "        '''\n",
    "        Takes a dictionary, path to data, and class of data at that path.\n",
    "        NOTE: THIS WILL NOT WORK FOR ANY OTHER DATA BECAUSE OF DIRECTORY STRUCTURE\n",
    "        Places the text, a dictionary of bagOfWords and the true class of the data\n",
    "        in the dictioary passed into this function\n",
    "        '''\n",
    "        for dir_entry in os.listdir(directory):\n",
    "            dir_entry_path = os.path.join(directory, dir_entry)\n",
    "            if os.path.isfile(dir_entry_path):\n",
    "                with open(dir_entry_path,encoding='utf8',errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "                    data.update({dir_entry_path: {'text': text, 'freqWords': bagOfWords(text), 'trueClass': trueClass}})\n",
    "    \n",
    "        \n",
    "    def vocabSet(self, dataSet):\n",
    "        '''\n",
    "        Called inside train method, takes the training data\n",
    "        and returns a set of all words in the data. The set\n",
    "        is used to initialize the weights of each word before\n",
    "        training begins when weights are computed. These words\n",
    "        become the keys for weights dictionary\n",
    "        '''\n",
    "        vocab = []\n",
    "        for i in dataSet:\n",
    "            for word in dataSet[i]['freqWords']:\n",
    "                if word not in vocab:\n",
    "                    vocab.append(word)\n",
    "        return vocab\n",
    "\n",
    "    def computeWeights(self, dataSet, weights, lr, epochs):\n",
    "        '''\n",
    "        Takes the dataset, weights, learning rate, and iterations\n",
    "        as input. This is the core of the perceptron algorithm.\n",
    "        This is done during training and is called from train()\n",
    "        Iterates through the dataset and its word frequency\n",
    "        dictionary and takes the sum of weights of the words times\n",
    "        the frequency of the words in email. If the word in the email\n",
    "        is not initialized then it is added and initialized to 0.\n",
    "        For each email if the sum of weights is greater than 0 then\n",
    "        the perceptron output is 1. Then update the weights of words\n",
    "        by multiplying the learning rate times difference between \n",
    "        prediction and truth and also the frequency of the word.\n",
    "        '''\n",
    "        for i in range(epochs):\n",
    "            for data in dataSet:\n",
    "                sumWeights = weights['w0']\n",
    "                for j in dataSet[data]['freqWords']:\n",
    "                    if j not in weights:\n",
    "                        weights[j] = 0\n",
    "                    sumWeights += weights[j] * dataSet[data]['freqWords'][j]\n",
    "                perceptronOutput = 0\n",
    "                if sumWeights > 0:\n",
    "                    perceptronOutput = 1\n",
    "                targetVal = 0\n",
    "                if dataSet[data]['trueClass'] == 'spam':\n",
    "                    targetVal = 1\n",
    "                for k in dataSet[data]['freqWords']:\n",
    "                    weights[k] += float(lr) * float(targetVal - perceptronOutput) * \\\n",
    "                    float(dataSet[data]['freqWords'][k])\n",
    "    \n",
    "    def classify(self, data, weights):\n",
    "        '''\n",
    "        Takes an instance of the data and the dictionary\n",
    "        of weights. Takes the sum of products of weight\n",
    "        of word and frequency of word in dataset. If the\n",
    "        word is new initialize weight of the word to zero.\n",
    "        If the sum is greater than 0 classifies as spam\n",
    "        and as ham if sum is less than 0.\n",
    "        Called in test method\n",
    "        '''\n",
    "        sumWeights = weights['w0']\n",
    "        for i in data['freqWords']:\n",
    "            if i not in weights:\n",
    "                weights[i] = 0\n",
    "            sumWeights += weights[i] * data['freqWords'][i]\n",
    "        if sumWeights > 0:\n",
    "            return 1 # spam\n",
    "        else:\n",
    "            return 0 # ham\n",
    "    \n",
    "    def preTrain(self):\n",
    "        '''\n",
    "        After getData() is called and self.trainingSet has the data,\n",
    "        this functiona takes that training data and splits it into \n",
    "        70% training and 30% testing for hyperparameter training\n",
    "        The data is store in self.trainSet70 and self.trainSet30\n",
    "        '''\n",
    "        lenTrain70= round(len(self.trainingSet.keys()) * 0.7) \n",
    "        lenTrain30 = len(self.trainingSet.keys()) - lenTrain70\n",
    "        \n",
    "        trainingSetKeys = list(self.trainingSet.keys())\n",
    "        train70Keys = trainingSetKeys[-lenTrain70:]\n",
    "        train30Keys = trainingSetKeys[:lenTrain30]\n",
    "\n",
    "        for i in train70Keys:\n",
    "            self.trainSet70[i] = self.trainingSet[i] \n",
    "            \n",
    "        for j in train30Keys:\n",
    "            self.trainSet30[j] = self.trainingSet[j]\n",
    "        \n",
    "        \n",
    "    def train(self, trainSet, lr, epochs):\n",
    "        '''\n",
    "        Takes training data set, learning rate, and epochs\n",
    "        as input. Gets a vocab set and initialzies each\n",
    "        words weight as zero. Then calls compute weights\n",
    "        to update the weights\n",
    "        '''\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "          \n",
    "        trainingSetVocab = self.vocabSet(trainSet)\n",
    "        \n",
    "        for i in trainingSetVocab:\n",
    "            self.weights[i] = 0.0\n",
    "        \n",
    "        self.computeWeights(self.trainingSet, self.weights, self.lr, self.epochs)\n",
    "        \n",
    "    def test(self, testSet):\n",
    "        '''\n",
    "        Takes a test data set as input and classifies each\n",
    "        instance of the data if the guess matches the true\n",
    "        class of the instance then increments correctGuesses\n",
    "        Returns correctGuess and percentage of accuracy\n",
    "        '''\n",
    "        correctGuesses = 0\n",
    "        for i in testSet:\n",
    "            guess = self.classify(testSet[i], self.weights)\n",
    "            if guess == 1:\n",
    "                testSet[i]['learnedClass'] = 'spam'\n",
    "                if testSet[i]['trueClass'] == testSet[i]['learnedClass']:\n",
    "                    correctGuesses += 1\n",
    "            if guess == 0:\n",
    "                testSet[i]['learnedClass'] = 'ham'\n",
    "                if testSet[i]['trueClass'] == testSet[i]['learnedClass']:\n",
    "                    correctGuesses += 1\n",
    "        return correctGuesses, (float(correctGuesses) / float(len(testSet)) * 100.0)\n",
    "\n",
    "        \n",
    "def main(trainDir, testDir):\n",
    "    lrs = [0.01, 0.03, 0.05, 0.1, 0.15]\n",
    "    epochs = [5, 75, 100]\n",
    "    maxAccuracy = 0\n",
    "    bestEpoch = 0\n",
    "    bestLr = 0\n",
    "    \n",
    "    perceptron = Perceptron()\n",
    "    perceptron.getData(perceptron.trainingSet, trainDir + \"/spam\", \"spam\")\n",
    "    perceptron.getData(perceptron.trainingSet, trainDir + \"/ham\", \"ham\")\n",
    "    perceptron.getData(perceptron.testSet, testDir + \"/spam\", \"spam\")\n",
    "    perceptron.getData(perceptron.testSet, testDir + \"/ham\", \"ham\")\n",
    "    \n",
    "    print(\"\\nData initialized:\")\n",
    "    print(\"Started training...\")\n",
    "    \n",
    "    '''\n",
    "    Hyper parameter tuning. Selects the best \n",
    "    epochs and learning rate based on accuracy\n",
    "    and those are used to train the full data set\n",
    "    '''\n",
    "    perceptron.preTrain()\n",
    "    for i in range(len(epochs)):\n",
    "        for j in range(len(lrs)):\n",
    "            perceptron.train(perceptron.trainSet70, lrs[j], epochs[i])\n",
    "            guess, acc = perceptron.test(perceptron.trainSet30)\n",
    "            if acc > maxAccuracy:\n",
    "                maxAccuracy = acc\n",
    "                bestEpoch = epochs[i]\n",
    "                bestLr = lrs[j]\n",
    "    \n",
    "    print(\"\\nBest results from hyperparameter tuning:\")            \n",
    "    print('Best Acc: ', maxAccuracy)\n",
    "    print('best Epoch: ', bestEpoch)\n",
    "    print('Best lr: ', bestLr)\n",
    "    \n",
    "    perceptron.train(perceptron.trainingSet, bestLr, bestEpoch)\n",
    "    guess, acc = perceptron.test(perceptron.testSet)\n",
    "    print(\"\\nRESULTS FROM ENTIRE DATASET:\")\n",
    "    print (\"\\nLearning rate: %.4f\" % float(perceptron.lr))\n",
    "    print (\"Number of epochs: %d\" % int(perceptron.epochs))\n",
    "    print (\"Emails classified correctly: %d/%d\" % (guess, len(perceptron.testSet)))\n",
    "    print (\"Accuracy: %.4f%%\" % (float(guess) / float(len(perceptron.testSet)) * 100.0))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    print(\"DATASET 1---------------------------------\")\n",
    "    main('data/dataset 1/train', 'data/dataset 1/test')\n",
    "    print(\"\\nDATASET 2---------------------------------\")\n",
    "    main('data/dataset 2/train', 'data/dataset 2/test')\n",
    "    print(\"\\nDATASET 3---------------------------------\")\n",
    "    main('data/dataset 3/train', 'data/dataset 3/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
